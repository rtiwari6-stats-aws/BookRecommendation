import numpy as np
import pandas as pd


books = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')


books.head()


ratings.head()


users.head()


print("Books Shape: " ,books.shape )
print("Ratings Shape: " ,ratings.shape )
print("Users Shape: " ,users.shape )


books_ratings=books.merge(ratings,on="ISBN")
books_ratings.head()


# popularity based

#logic for top 10 books
n = 10 # top 10 books
reviews_count=books_ratings.groupby("Book-Title")["Book-Rating"].count().reset_index()
reviews_count.rename(columns={"Book-Rating":"NumberOfVotes"},inplace=True)
reviews_average=books_ratings.groupby("Book-Title")["Book-Rating"].mean().reset_index()
reviews_average.rename(columns={"Book-Rating":"AverageRatings"},inplace=True)

popularBooks=reviews_count.merge(reviews_average,on="Book-Title")
popularBooks=popularBooks[popularBooks["AverageRatings"] >=9]
popularBooks=popularBooks.sort_values(by="NumberOfVotes",ascending=False).head(n)

# top 10 books
popularBooks


#similarity on books

# use DistilBERT

#%pip install sentence-transformers
#%pip install transformers
# Machine learning
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from transformers import DistilBertTokenizer, DistilBertModel # lighter version of BertTokenizer, BertModel
import time
from scipy.spatial.distance import cosine, pdist, squareform
from scipy.sparse import csr_matrix, coo_matrix, hstack
from scipy.sparse import identity
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

book_titles = books['Book-Title']##.tolist()

# REMOVE!!!
book_titles = book_titles.head(1000).tolist()

print('print(len(books)):', len(book_titles))
# accepts a list of books


def calculate_embeddings(book_titles):

    torch.set_num_threads(2)

    if torch.cuda.is_available():
        print("GPU is available")
        print("GPU Name:", torch.cuda.get_device_name(0))
    else:
        print("GPU is not available, using CPU")
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    model = DistilBertModel.from_pretrained('distilbert-base-uncased')

    # Start timing
    start_time = time.time()
    batch_size = 64

    # model = model.to(torch.device("cuda")) #  Use GPU for the computation

    pooled_embeddings = []
    for i in range(0, len(book_titles), batch_size):
        batch_titles = book_titles[i:i+batch_size]
        inputs = tokenizer(batch_titles, padding=True, truncation=True, return_tensors="pt", 
                           max_length=128, return_attention_mask=True)

        # inputs = {key: value.to(torch.device("cuda")) for key, value in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)

        batch_embeddings = outputs.last_hidden_state.mean(dim=1)
        pooled_embeddings.append(batch_embeddings.cpu())

        # Free up memory after processing each batch
        del inputs, outputs, batch_embeddings
        # torch.cuda.empty_cache()

    # Concatenate all batch embeddings
    pooled_embeddings = torch.cat(pooled_embeddings, dim=0)
    elapsed_time = time.time() - start_time
    print(f"Elapsed time: {elapsed_time} seconds")
    # Free up memory
    # torch.cuda.empty_cache()
    return pooled_embeddings


book_embeddings = calculate_embeddings(book_titles)
print(book_embeddings[:5])

# take a test book and compute it's similarity with every other book

test_book = 'Shambhala: Sacred Path of the Warrior'
test_embeddings = calculate_embeddings([test_book])
print(test_embeddings)

# compute cosine distance between test book and all other books
cosine_distances = []
# cos_1 = torch.nn.CosineSimilarity(dim=0)
max_tensor = None
i = 0
recommended_title = None
for book_embedding in book_embeddings:
    cosine_distance = torch.nn.functional.cosine_similarity(test_embeddings[0], 
                                                                            book_embedding, dim=0)
    cosine_distances.append(cosine_distance)
    if max_tensor is None:
        max_tensor = cosine_distance
        recommended_title = book_titles[i]        
    elif torch.gt(cosine_distance, max_tensor):
        max_tensor = cosine_distance
        recommended_title = book_titles[i]
    i = i + 1
    
print(max_tensor)
print(recommended_title)




